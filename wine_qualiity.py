# -*- coding: utf-8 -*-
"""wine qualiity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TTBdlBEyhDMuzMD3ktt4ML4qAMjaiMXh
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb

import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn import metrics
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

df=pd.read_csv('/content/winequality-red.csv')

df

print(df.head())

df.info()

df.describe().T

"""Exploratory Data Analysis"""

df.isnull().sum()

for col in df.columns:
  if df[col].isnull().sum() > 0:
    df[col] = df[col].fillna(df[col].mean())

df.isnull().sum().sum()

df.notnull()

df.duplicated()

df.duplicated().sum()

df.head()

df.tail()

print(df.columns)

print(df.shape)

print(df.dtypes)

print(df.ndim)

print(df.size)

# number of rows & columns in the dataset
df.shape

df.shape

print(df.nunique())

print(df.count())

print(df.max())

print(df.min())

print(df.mean())

print(df.median())

print(df.std())

print(df.var())

print(df)

"""histogram

"""

df.hist(bins=20, figsize=(10, 10))
plt.show()

# number of values for each quality
import seaborn as sns # Import seaborn with the alias 'sns'
sns.catplot(x='quality', data = df, kind = 'count') # Assuming wine_dataset should be df

# volatile acidity vs Quality
plot = plt.figure(figsize=(5,5))
sns.barplot(x='quality', y = 'volatile acidity', data = df) # Replace 'wine_dataset' with 'df'

plt.bar(df['quality'], df['alcohol'])
plt.xlabel('quality')
plt.ylabel('alcohol')
plt.show()

"""here are times the data provided to us contains redundant features they do not help with increasing the model’s performance that is why we remove them before using them to train our model.

Correlation

Positive Correlation
Negative Correlation
"""

correlation = df.corr()

# constructing a heatmap to understand the correlation between the columns
plt.figure(figsize=(10,10))
sns.heatmap(correlation, cbar=True, square=True, fmt = '.1f', annot = True, annot_kws={'size':8}, cmap = 'Blues')

"""Data Preprocessing"""

# separate the data and Label
X = df.drop('quality',axis=1)

print(X)

"""Label Binarizaton"""

Y = df['quality'].apply(lambda y_value: 1 if y_value>=7 else 0)

print(Y)

"""Train & Test Split"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)

print(Y.shape, Y_train.shape, Y_test.shape)

"""Model Training:

Random Forest Classifier

"""

model = RandomForestClassifier()

model.fit(X_train, Y_train)

"""Accuracy Score"""

# accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy : ', test_data_accuracy)

"""Building a Predictive System

"""

input_data = (7.5,0.5,0.36,6.1,0.071,17.0,102.0,0.9978,3.35,0.8,10.5)

# changing the input data to a numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the data as we are predicting the label for only one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

prediction = model.predict(input_data_reshaped)
print(prediction)

if (prediction[0]==1):
  print('Good Quality Wine')
else:
  print('Bad Quality Wine')

"""Model Development

Let’s our data for training and splitting it into training and validation data so, that we can select which model’s performance is best as per the use case. We will train some of the state of the art machine learning classification models and then select best out of them using validation data.
"""

df['best quality'] = [1 if x > 5 else 0 for x in df.quality]

"""We have a column with object data type as well let’s replace it with the 0 and 1 as there are only two categories."""

df.replace({'white': 1, 'red': 0}, inplace=True)

"""After segregating features and the target variable from the dataset we will split it into 80:20 ratio for model selection."""

features = df.drop(['quality', 'best quality'], axis=1) # Define 'features' first
features = features.fillna(features.mean()) # Now you can use 'features'
target = df['best quality']

xtrain, xtest, ytrain, ytest = train_test_split(
    features, target, test_size=0.2, random_state=40)

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')  # Or another strategy like 'median'
xtrain = imputer.fit_transform(xtrain)
xtest = imputer.transform(xtest)

xtrain.shape, xtest.shape

"""Normalization"""

norm = MinMaxScaler()
xtrain = norm.fit_transform(xtrain)
xtest = norm.transform(xtest)

"""As the data has been prepared completely let’s train some state of the art machine learning model on it."""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

models = [LogisticRegression(),GaussianNB(), RandomForestClassifier(),KNeighborsClassifier(),DecisionTreeClassifier(), SVC(kernel='rbf')]

for i in range(3):
    models[i].fit(xtrain, ytrain)

    print(f'{models[i]} : ')
    print('Training Accuracy : ', metrics.roc_auc_score(ytrain, models[i].predict(xtrain)))
    print('Validation Accuracy : ', metrics.roc_auc_score(
        ytest, models[i].predict(xtest)))
    print()

"""Model Evaluation

Confusion Matrix Display
"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Assuming 'models[1]' is your trained classifier
cm = confusion_matrix(ytest, models[1].predict(xtest))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=models[1].classes_) # Assuming your model has a 'classes_' attribute
disp.plot()
plt.show()

"""classification_report"""

print(metrics.classification_report(ytest, models[1].predict(xtest)))

from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt

# Assuming 'models[1]' is your trained classifier and 'xtest', 'ytest' are your test data
y_scores = models[1].predict_proba(xtest)[:, 1]  # Get predicted probabilities for the positive class
fpr, tpr, thresholds = roc_curve(ytest, y_scores)

# Now you can use fpr, tpr, and thresholds to plot the ROC curve or further analysis.
# For example, to plot the ROC curve:
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score # Corrected the typo in the module name
from sklearn.metrics import confusion_matrix, classification_report, roc_curve
import matplotlib.pyplot as plt

accuracy = accuracy_score(ytest, models[1].predict(xtest))
precision = precision_score(ytest, models[1].predict(xtest))
recall = recall_score(ytest, models[1].predict(xtest))
f1 = f1_score(ytest, models[1].predict(xtest))
roc_auc = roc_auc_score(ytest, models[1].predict(xtest))

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"ROC AUC Score: {roc_auc}")